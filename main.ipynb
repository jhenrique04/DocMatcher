{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_reader import PDFReader\n",
    "from preprocessor import Preprocessor\n",
    "from bert_model import BertModel\n",
    "from analyzer import Analyzer\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vec(words, model):\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "def process_documents(document_paths, pdf_reader, preprocessor, bert_model, word2vec_model):\n",
    "    document_analyses = []\n",
    "    embeddings = []\n",
    "\n",
    "    for path in document_paths:\n",
    "        text = pdf_reader.extract_text(path)\n",
    "        processed_text = preprocessor.process(text)\n",
    "\n",
    "        # Detailed analysis\n",
    "        analysis_details = preprocessor.analyze_details(processed_text)\n",
    "        complexity_score = preprocessor.analyze_complexity(processed_text)\n",
    "        style_score = preprocessor.analyze_style(processed_text)\n",
    "        vocabulary_diversity = preprocessor.analyze_vocabulary(processed_text)\n",
    "\n",
    "        # Extract key-terms\n",
    "        key_terms = preprocessor.extract_key_terms(processed_text)\n",
    "\n",
    "        # Calculate topics\n",
    "        topics = preprocessor.analyze_topics(processed_text)\n",
    "\n",
    "        analysis = {\n",
    "            \"details\": analysis_details,\n",
    "            \"complexity\": complexity_score,\n",
    "            \"style\": style_score,\n",
    "            \"vocabulary_diversity\": vocabulary_diversity,\n",
    "            \"key_terms\": key_terms,\n",
    "            \"topics\": topics\n",
    "        }\n",
    "\n",
    "        bert_embedding = bert_model.get_embeddings(processed_text)\n",
    "        w2v_embedding = document_to_vec(processed_text, word2vec_model)\n",
    "        combined_embedding = torch.from_numpy(np.concatenate(\n",
    "            (bert_embedding.detach().numpy(), w2v_embedding)))\n",
    "\n",
    "        embeddings.append(combined_embedding)\n",
    "        document_analyses.append((path, analysis))\n",
    "\n",
    "    return embeddings, document_analyses\n",
    "\n",
    "\n",
    "def generate_feedback(test_embedding, train_embeddings):\n",
    "    distances = [torch.dist(test_embedding, train_emb, 2).item()\n",
    "                 for train_emb in train_embeddings]\n",
    "    avg_distance = sum(distances) / len(distances)\n",
    "    if avg_distance > 0.21:\n",
    "        return \"The document is not aligned with the training standards.\"\n",
    "    else:\n",
    "        return \"The document is aligned with the training standards.\"\n",
    "\n",
    "\n",
    "def get_document_paths(directory):\n",
    "    doc_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                doc_paths.append(os.path.join(root, file))\n",
    "    return doc_paths\n",
    "\n",
    "\n",
    "def calculate_benchmarks(train_analyses):\n",
    "    # Calculating averages and standard deviations\n",
    "    sentiment_scores = [analysis['details']['sentiment']\n",
    "                        ['compound'] for _, analysis in train_analyses]\n",
    "    num_sentences = [analysis['details']['num_sentences']\n",
    "                     for _, analysis in train_analyses]\n",
    "\n",
    "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "    std_sentiment = np.std(sentiment_scores)\n",
    "    avg_num_sentences = sum(num_sentences) / len(num_sentences)\n",
    "    std_num_sentences = np.std(num_sentences)\n",
    "\n",
    "    return {\n",
    "        \"avg_sentiment\": avg_sentiment,\n",
    "        \"std_sentiment\": std_sentiment,\n",
    "        \"avg_num_sentences\": avg_num_sentences,\n",
    "        \"std_num_sentences\": std_num_sentences\n",
    "    }\n",
    "\n",
    "\n",
    "def main(model_path):\n",
    "    pdf_reader = PDFReader()\n",
    "    preprocessor = Preprocessor()\n",
    "    bert_model = BertModel('jackaduma/SecBERT')\n",
    "    analyzer = Analyzer()\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(\n",
    "        model_path, binary=False)\n",
    "\n",
    "    train_doc_paths = get_document_paths('train/')\n",
    "    test_doc_paths = get_document_paths('test/')\n",
    "\n",
    "    train_embeddings, train_analyses = process_documents(\n",
    "        train_doc_paths, pdf_reader, preprocessor, bert_model, word2vec_model)\n",
    "    test_embeddings, test_analyses = process_documents(\n",
    "        test_doc_paths, pdf_reader, preprocessor, bert_model, word2vec_model)\n",
    "\n",
    "    benchmarks = calculate_benchmarks(train_analyses)\n",
    "\n",
    "    for test_embedding, test_analysis in zip(test_embeddings, test_analyses):\n",
    "        test_path = test_analysis[0]  # Doc path\n",
    "        analysis_data = test_analysis[1]\n",
    "\n",
    "        print(f\"\\nAnalyzing document: {test_path}\")\n",
    "\n",
    "        # Euclidian distance\n",
    "        avg_distance = analyzer.average_distance_to_train_docs(\n",
    "            test_embedding, train_embeddings)\n",
    "        print(f\"Average distance to training documents = {\n",
    "              avg_distance:.3f}\")\n",
    "\n",
    "        # Similarity analysis\n",
    "        analysis_result = analyzer.analyze(test_embedding, train_embeddings)\n",
    "        print(f\"Average cosine similarity: {\n",
    "              analysis_result['average_similarity']:.3f}\")\n",
    "\n",
    "        # Embedding-based feedback\n",
    "        feedback = generate_feedback(test_embedding, train_embeddings)\n",
    "\n",
    "        # Detailed analysis feedback\n",
    "        detailed_feedback = f\"Detailed Analysis: Sentiment - {\n",
    "            analysis_data['details']['sentiment']}, Number of Sentences - {analysis_data['details']['num_sentences']}\"\n",
    "        feedback += f\"\\n{detailed_feedback}\"\n",
    "\n",
    "        # Compare benchmarks and add feedbacks\n",
    "        sentiment_diff = analysis_data['details']['sentiment']['compound'] - \\\n",
    "            benchmarks['avg_sentiment']\n",
    "        num_sentences_diff = analysis_data['details']['num_sentences'] - \\\n",
    "            benchmarks['avg_num_sentences']\n",
    "\n",
    "        if abs(sentiment_diff) > benchmarks['std_sentiment']:\n",
    "            feedback += f\"\\nThe sentiment of this document is significantly {\n",
    "                'positive' if sentiment_diff > 0 else 'negative'} compared to the training documents.\"\n",
    "        if abs(num_sentences_diff) > benchmarks['std_num_sentences']:\n",
    "            feedback += f\"\\nThis document has {'more' if num_sentences_diff >\n",
    "                                               0 else 'fewer'} sentences than the average of the training documents.\"\n",
    "\n",
    "        # Add new feedbacks based on the analysis\n",
    "        feedback += f\"\\nText Complexity (Flesch-Kincaid): {\n",
    "            analysis_data['complexity']}\"\n",
    "        feedback += f\"\\nWriting Style: {\n",
    "            'Passive' if analysis_data['style'] > 0 else 'Active'}\"\n",
    "        feedback += f\"\\nVocabulary Diversity: {\n",
    "            analysis_data['vocabulary_diversity']:.2f}\"\n",
    "        feedback += f\"\\nIdentified Key Terms: {\n",
    "            ', '.join(analysis_data['key_terms'])}\"\n",
    "        feedback += f\"\\nMain Topics: {', '.join(analysis_data['topics'])}\"\n",
    "\n",
    "        # Print final feedback\n",
    "        print(f\"Final Feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at jackaduma/SecBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing document: test/documentCategory1/test2.pdf\n",
      "Average distance to training documents = 0.095\n",
      "Average cosine similarity: 0.982\n",
      "Final Feedback: The document is aligned with the training standards.\n",
      "Detailed Analysis: Sentiment - {'neg': 0.0, 'neu': 0.999, 'pos': 0.001, 'compound': 0.128}, Number of Sentences - 190\n",
      "Text Complexity (Flesch-Kincaid): 21.5\n",
      "Writing Style: Active\n",
      "Vocabulary Diversity: 0.34\n",
      "Identified Key Terms: segurança, ataque, risco, vulnerabilidade\n",
      "Main Topics: ETIR, Segurança, Informação, incidente, serviço\n",
      "\n",
      "Analyzing document: test/documentCategory1/test1.pdf\n",
      "Average distance to training documents = 0.100\n",
      "Average cosine similarity: 0.982\n",
      "Final Feedback: The document is aligned with the training standards.\n",
      "Detailed Analysis: Sentiment - {'neg': 0.009, 'neu': 0.985, 'pos': 0.006, 'compound': -0.7556}, Number of Sentences - 206\n",
      "The sentiment of this document is significantly negative compared to the training documents.\n",
      "Text Complexity (Flesch-Kincaid): 17.4\n",
      "Writing Style: Active\n",
      "Vocabulary Diversity: 0.29\n",
      "Identified Key Terms: segurança, risco, firewall, vulnerabilidade\n",
      "Main Topics: Segurança, TRABALHO, incidente, cibernético, Informação\n",
      "\n",
      "Analyzing document: test/documentCategory3/test6.pdf\n",
      "Average distance to training documents = 0.110\n",
      "Average cosine similarity: 0.981\n",
      "Final Feedback: The document is aligned with the training standards.\n",
      "Detailed Analysis: Sentiment - {'neg': 0.006, 'neu': 0.992, 'pos': 0.002, 'compound': -0.7717}, Number of Sentences - 319\n",
      "The sentiment of this document is significantly negative compared to the training documents.\n",
      "Text Complexity (Flesch-Kincaid): 21.2\n",
      "Writing Style: Active\n",
      "Vocabulary Diversity: 0.27\n",
      "Identified Key Terms: risco\n",
      "Main Topics: Mellon, BNY, ativo, capital, Patrimônio\n",
      "\n",
      "Analyzing document: test/documentCategory3/test5.pdf\n",
      "Average distance to training documents = 0.081\n",
      "Average cosine similarity: 0.983\n",
      "Final Feedback: The document is aligned with the training standards.\n",
      "Detailed Analysis: Sentiment - {'neg': 0.002, 'neu': 0.993, 'pos': 0.006, 'compound': 0.5719}, Number of Sentences - 208\n",
      "Text Complexity (Flesch-Kincaid): 28.8\n",
      "Writing Style: Active\n",
      "Vocabulary Diversity: 0.31\n",
      "Identified Key Terms: risco\n",
      "Main Topics: risco, capital, gerenciamento, mercado, PAN\n",
      "\n",
      "Analyzing document: test/documentCategory2/test4.pdf\n",
      "Average distance to training documents = 0.096\n",
      "Average cosine similarity: 0.982\n",
      "Final Feedback: The document is aligned with the training standards.\n",
      "Detailed Analysis: Sentiment - {'neg': 0.003, 'neu': 0.99, 'pos': 0.007, 'compound': 0.5622}, Number of Sentences - 505\n",
      "Text Complexity (Flesch-Kincaid): 25.2\n",
      "Writing Style: Active\n",
      "Vocabulary Diversity: 0.25\n",
      "Identified Key Terms: segurança, risco, vulnerabilidade\n",
      "Main Topics: IFS, informação, Informação, nº, EDUCAÇÃO\n",
      "\n",
      "Analyzing document: test/documentCategory2/test3.pdf\n",
      "Average distance to training documents = 0.083\n",
      "Average cosine similarity: 0.983\n",
      "Final Feedback: The document is aligned with the training standards.\n",
      "Detailed Analysis: Sentiment - {'neg': 0.002, 'neu': 0.992, 'pos': 0.006, 'compound': 0.938}, Number of Sentences - 741\n",
      "Text Complexity (Flesch-Kincaid): 37.1\n",
      "Writing Style: Passive\n",
      "Vocabulary Diversity: 0.23\n",
      "Identified Key Terms: segurança, ataque, criptografia, risco, vulnerabilidade\n",
      "Main Topics: Informação, Konecta, informação, acesso, Segurança\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main('word2Vec_models/your_word22Vec_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
